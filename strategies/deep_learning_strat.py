"""
Advanced Deep Learning Trading Strategy for FOREX TRADING BOT
Neural network-based trading with reinforcement learning and ensemble methods
"""

import logging
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from typing import Dict, List, Optional, Union, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import time
import warnings
from collections import defaultdict, deque
import joblib
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import talib
from scipy import stats
import json
from pathlib import Path
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading

# Suppress warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class Action(Enum):
    BUY = "buy"
    SELL = "sell"
    HOLD = "hold"
    CLOSE = "close"

class MarketRegime(Enum):
    TRENDING_UP = "trending_up"
    TRENDING_DOWN = "trending_down"
    RANGING = "ranging"
    VOLATILE = "volatile"
    BREAKOUT = "breakout"

class ModelType(Enum):
    LSTM = "lstm"
    GRU = "gru"
    TRANSFORMER = "transformer"
    CNN_LSTM = "cnn_lstm"
    ENSEMBLE = "ensemble"

@dataclass
class TradingSignal:
    """Trading signal generated by deep learning model"""
    symbol: str
    action: Action
    confidence: float
    price: float
    timestamp: datetime
    stop_loss: float
    take_profit: float
    model_type: ModelType
    features_used: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ModelConfig:
    """Configuration for deep learning models"""
    # Model architecture
    model_type: ModelType = ModelType.ENSEMBLE
    sequence_length: int = 60
    prediction_horizon: int = 5
    hidden_size: int = 128
    num_layers: int = 3
    dropout_rate: float = 0.2
    
    # Training parameters
    learning_rate: float = 0.001
    batch_size: int = 64
    epochs: int = 100
    early_stopping_patience: int = 10
    validation_split: float = 0.2
    
    # Feature engineering
    technical_indicators: List[str] = field(default_factory=lambda: [
        'RSI', 'MACD', 'BBANDS', 'ATR', 'ADX', 'CCI', 'MFI', 'OBV',
        'STOCH', 'WILLR', 'ULTOSC', 'ROC', 'PPO', 'MOM'
    ])
    price_features: List[str] = field(default_factory=lambda: [
        'open', 'high', 'low', 'close', 'volume'
    ])
    statistical_features: List[str] = field(default_factory=lambda: [
        'returns', 'volatility', 'skew', 'kurtosis', 'zscore'
    ])
    
    # Risk management
    min_confidence: float = 0.65
    max_position_size: float = 0.1
    risk_reward_ratio: float = 1.5
    
    # Advanced features
    enable_attention: bool = True
    enable_batch_norm: bool = True
    enable_residual: bool = True
    enable_autoencoder: bool = False

@dataclass
class TrainingResult:
    """Training results and metrics"""
    model_type: ModelType
    training_loss: List[float]
    validation_loss: List[float]
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    training_time: float
    best_epoch: int
    feature_importance: Dict[str, float]

class TimeSeriesDataset(Dataset):
    """PyTorch Dataset for time series data"""
    
    def __init__(self, features: np.ndarray, targets: np.ndarray, sequence_length: int):
        self.features = features
        self.targets = targets
        self.sequence_length = sequence_length
        
    def __len__(self):
        return len(self.features) - self.sequence_length
    
    def __getitem__(self, idx):
        features_seq = self.features[idx:idx + self.sequence_length]
        target = self.targets[idx + self.sequence_length]
        return torch.FloatTensor(features_seq), torch.FloatTensor([target])

class LSTMModel(nn.Module):
    """Advanced LSTM Model with attention mechanism"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, 
                 output_size: int, dropout: float, enable_attention: bool = True,
                 enable_batch_norm: bool = True):
        super(LSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.enable_attention = enable_attention
        self.enable_batch_norm = enable_batch_norm
        
        # LSTM layers
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )
        
        # Batch normalization
        if enable_batch_norm:
            self.batch_norm = nn.BatchNorm1d(hidden_size * 2)
        
        # Attention mechanism
        if enable_attention:
            self.attention = nn.Sequential(
                nn.Linear(hidden_size * 2, hidden_size),
                nn.Tanh(),
                nn.Linear(hidden_size, 1),
                nn.Softmax(dim=1)
            )
        
        # Output layers
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size),
            nn.Sigmoid()  # Output between 0 and 1
        )
        
    def forward(self, x):
        # LSTM forward pass
        lstm_out, (hidden, cell) = self.lstm(x)
        
        if self.enable_batch_norm:
            lstm_out = self.batch_norm(lstm_out.transpose(1, 2)).transpose(1, 2)
        
        # Attention mechanism
        if self.enable_attention:
            attention_weights = self.attention(lstm_out)
            context_vector = torch.sum(attention_weights * lstm_out, dim=1)
        else:
            # Use last hidden state
            context_vector = lstm_out[:, -1, :]
        
        # Fully connected layers
        output = self.fc_layers(context_vector)
        return output

class GRUModel(nn.Module):
    """GRU Model with residual connections"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int,
                 output_size: int, dropout: float, enable_residual: bool = True):
        super(GRUModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.enable_residual = enable_residual
        
        # GRU layers
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )
        
        # Residual connection
        if enable_residual:
            self.residual = nn.Linear(input_size, hidden_size * 2)
        
        # Output layers
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, output_size),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        gru_out, hidden = self.gru(x)
        
        # Residual connection
        if self.enable_residual:
            residual = self.residual(x[:, -1, :])
            gru_out = gru_out[:, -1, :] + residual
        else:
            gru_out = gru_out[:, -1, :]
        
        output = self.fc_layers(gru_out)
        return output

class TransformerModel(nn.Module):
    """Transformer Model for time series forecasting"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int,
                 output_size: int, dropout: float, num_heads: int = 8):
        super(TransformerModel, self).__init__()
        
        self.input_projection = nn.Linear(input_size, hidden_size)
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # Output layers
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # Input projection
        x = self.input_projection(x)
        
        # Transformer forward pass
        transformer_out = self.transformer(x)
        
        # Use last time step
        output = transformer_out[:, -1, :]
        output = self.fc_layers(output)
        return output

class CNNLSTMModel(nn.Module):
    """CNN-LSTM Hybrid Model"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int,
                 output_size: int, dropout: float):
        super(CNNLSTMModel, self).__init__()
        
        # CNN layers for feature extraction
        self.conv_layers = nn.Sequential(
            nn.Conv1d(input_size, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Conv1d(64, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.AdaptiveAvgPool1d(1)
        )
        
        # LSTM layers
        self.lstm = nn.LSTM(
            input_size=32,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout
        )
        
        # Output layers
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # CNN feature extraction
        x = x.transpose(1, 2)  # Convert to (batch, features, sequence)
        cnn_features = self.conv_layers(x).squeeze(-1)
        
        # Prepare for LSTM (add sequence dimension)
        cnn_features = cnn_features.unsqueeze(1)
        
        # LSTM forward pass
        lstm_out, (hidden, cell) = self.lstm(cnn_features)
        
        # Fully connected layers
        output = self.fc_layers(lstm_out[:, -1, :])
        return output

class AdvancedDeepLearningStrategy:
    """
    Advanced Deep Learning Trading Strategy with Multiple Neural Networks
    """
    
    def __init__(self, config: ModelConfig = None):
        self.config = config or ModelConfig()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Models
        self.models: Dict[ModelType, nn.Module] = {}
        self.scalers: Dict[str, StandardScaler] = {}
        self.optimizers: Dict[ModelType, optim.Optimizer] = {}
        self.schedulers: Dict[ModelType, optim.lr_scheduler._LRScheduler] = {}
        
        # Training history
        self.training_history: Dict[ModelType, TrainingResult] = {}
        self.feature_names: List[str] = []
        
        # Market data
        self.market_data: pd.DataFrame = None
        self.processed_features: pd.DataFrame = None
        
        # Ensemble weights
        self.ensemble_weights: Dict[ModelType, float] = {}
        
        # Thread safety
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=4)
        
        logger.info(f"DeepLearningStrategy initialized on device: {self.device}")
    
    def _initialize_models(self, input_size: int) -> None:
        """Initialize all deep learning models"""
        try:
            output_size = 3  # Buy, Sell, Hold probabilities
            
            if ModelType.LSTM in [self.config.model_type] or self.config.model_type == ModelType.ENSEMBLE:
                self.models[ModelType.LSTM] = LSTMModel(
                    input_size=input_size,
                    hidden_size=self.config.hidden_size,
                    num_layers=self.config.num_layers,
                    output_size=output_size,
                    dropout=self.config.dropout_rate,
                    enable_attention=self.config.enable_attention,
                    enable_batch_norm=self.config.enable_batch_norm
                ).to(self.device)
                
                self.optimizers[ModelType.LSTM] = optim.Adam(
                    self.models[ModelType.LSTM].parameters(),
                    lr=self.config.learning_rate,
                    weight_decay=1e-5
                )
                
                self.schedulers[ModelType.LSTM] = optim.lr_scheduler.ReduceLROnPlateau(
                    self.optimizers[ModelType.LSTM], 
                    patience=5, 
                    factor=0.5
                )
            
            if ModelType.GRU in [self.config.model_type] or self.config.model_type == ModelType.ENSEMBLE:
                self.models[ModelType.GRU] = GRUModel(
                    input_size=input_size,
                    hidden_size=self.config.hidden_size,
                    num_layers=self.config.num_layers,
                    output_size=output_size,
                    dropout=self.config.dropout_rate,
                    enable_residual=self.config.enable_residual
                ).to(self.device)
                
                self.optimizers[ModelType.GRU] = optim.Adam(
                    self.models[ModelType.GRU].parameters(),
                    lr=self.config.learning_rate
                )
            
            if ModelType.TRANSFORMER in [self.config.model_type] or self.config.model_type == ModelType.ENSEMBLE:
                self.models[ModelType.TRANSFORMER] = TransformerModel(
                    input_size=input_size,
                    hidden_size=self.config.hidden_size,
                    num_layers=self.config.num_layers,
                    output_size=output_size,
                    dropout=self.config.dropout_rate
                ).to(self.device)
                
                self.optimizers[ModelType.TRANSFORMER] = optim.Adam(
                    self.models[ModelType.TRANSFORMER].parameters(),
                    lr=self.config.learning_rate
                )
            
            if ModelType.CNN_LSTM in [self.config.model_type] or self.config.model_type == ModelType.ENSEMBLE:
                self.models[ModelType.CNN_LSTM] = CNNLSTMModel(
                    input_size=input_size,
                    hidden_size=self.config.hidden_size,
                    num_layers=self.config.num_layers,
                    output_size=output_size,
                    dropout=self.config.dropout_rate
                ).to(self.device)
                
                self.optimizers[ModelType.CNN_LSTM] = optim.Adam(
                    self.models[ModelType.CNN_LSTM].parameters(),
                    lr=self.config.learning_rate
                )
            
            logger.info(f"Initialized {len(self.models)} deep learning models")
            
        except Exception as e:
            logger.error(f"Model initialization failed: {e}")
            raise
    
    def prepare_features(self, market_data: pd.DataFrame) -> pd.DataFrame:
        """Prepare advanced features for deep learning models"""
        try:
            df = market_data.copy()
            
            # Basic price features
            df['returns'] = df['close'].pct_change()
            df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
            df['volatility'] = df['returns'].rolling(window=20).std()
            df['price_change'] = df['close'].diff()
            
            # Technical indicators
            if 'RSI' in self.config.technical_indicators:
                df['RSI'] = talib.RSI(df['close'], timeperiod=14)
            
            if 'MACD' in self.config.technical_indicators:
                macd, macd_signal, macd_hist = talib.MACD(df['close'])
                df['MACD'] = macd
                df['MACD_signal'] = macd_signal
                df['MACD_hist'] = macd_hist
            
            if 'BBANDS' in self.config.technical_indicators:
                upper, middle, lower = talib.BBANDS(df['close'], timeperiod=20)
                df['BB_upper'] = upper
                df['BB_middle'] = middle
                df['BB_lower'] = lower
                df['BB_width'] = (upper - lower) / middle
                df['BB_position'] = (df['close'] - lower) / (upper - lower)
            
            if 'ATR' in self.config.technical_indicators:
                df['ATR'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)
            
            if 'ADX' in self.config.technical_indicators:
                df['ADX'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)
            
            if 'CCI' in self.config.technical_indicators:
                df['CCI'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)
            
            if 'MFI' in self.config.technical_indicators:
                df['MFI'] = talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=14)
            
            if 'OBV' in self.config.technical_indicators:
                df['OBV'] = talib.OBV(df['close'], df['volume'])
            
            # Statistical features
            if 'skew' in self.config.statistical_features:
                df['returns_skew'] = df['returns'].rolling(window=20).skew()
            
            if 'kurtosis' in self.config.statistical_features:
                df['returns_kurtosis'] = df['returns'].rolling(window=20).kurtosis()
            
            if 'zscore' in self.config.statistical_features:
                df['price_zscore'] = (df['close'] - df['close'].rolling(window=20).mean()) / df['close'].rolling(window=20).std()
            
            # Price patterns
            df['price_momentum'] = df['close'] / df['close'].shift(5) - 1
            df['volume_momentum'] = df['volume'] / df['volume'].shift(5) - 1
            
            # Market regime features
            df['trend_strength'] = abs(df['close'] - df['close'].shift(20)) / df['ATR'].rolling(window=20).mean()
            df['volatility_regime'] = df['volatility'] / df['volatility'].rolling(window=100).mean()
            
            # Fill NaN values
            df = df.fillna(method='bfill').fillna(method='ffill')
            
            # Store feature names
            self.feature_names = [col for col in df.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'timestamp']]
            
            logger.info(f"Prepared {len(self.feature_names)} features for training")
            return df
            
        except Exception as e:
            logger.error(f"Feature preparation failed: {e}")
            raise
    
    def create_sequences(self, data: pd.DataFrame, target_col: str = 'returns') -> Tuple[np.ndarray, np.ndarray]:
        """Create sequences for time series forecasting"""
        try:
            feature_cols = self.feature_names + ['close', 'volume']
            features = data[feature_cols].values
            targets = data[target_col].values
            
            # Create sequences
            X, y = [], []
            for i in range(self.config.sequence_length, len(features) - self.config.prediction_horizon):
                X.append(features[i - self.config.sequence_length:i])
                
                # Create multi-class target: 0=SELL, 1=HOLD, 2=BUY
                future_return = (data['close'].iloc[i + self.config.prediction_horizon] / data['close'].iloc[i] - 1)
                
                if future_return > 0.002:  # 0.2% threshold for buy
                    target = 2  # BUY
                elif future_return < -0.002:  # -0.2% threshold for sell
                    target = 0  # SELL
                else:
                    target = 1  # HOLD
                
                y.append(target)
            
            return np.array(X), np.array(y)
            
        except Exception as e:
            logger.error(f"Sequence creation failed: {e}")
            raise
    
    def train_models(self, market_data: pd.DataFrame) -> Dict[ModelType, TrainingResult]:
        """Train all deep learning models"""
        try:
            logger.info("Starting model training...")
            
            # Prepare features
            processed_data = self.prepare_features(market_data)
            self.market_data = processed_data
            
            # Create sequences
            X, y = self.create_sequences(processed_data)
            
            # Scale features
            self.scalers['feature'] = StandardScaler()
            X_scaled = self.scalers['feature'].fit_transform(
                X.reshape(-1, X.shape[-1])
            ).reshape(X.shape)
            
            # Split data
            split_idx = int(len(X) * (1 - self.config.validation_split))
            X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
            
            # Convert to PyTorch tensors
            train_dataset = TimeSeriesDataset(X_train, y_train, self.config.sequence_length)
            val_dataset = TimeSeriesDataset(X_val, y_val, self.config.sequence_length)
            
            train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size, shuffle=False)
            
            # Initialize models
            input_size = X_train.shape[-1]
            self._initialize_models(input_size)
            
            # Train each model
            training_results = {}
            for model_type, model in self.models.items():
                logger.info(f"Training {model_type.value} model...")
                result = self._train_single_model(
                    model_type, model, train_loader, val_loader
                )
                training_results[model_type] = result
            
            # Calculate ensemble weights based on validation performance
            self._calculate_ensemble_weights(training_results)
            
            self.training_history.update(training_results)
            logger.info("Model training completed successfully")
            
            return training_results
            
        except Exception as e:
            logger.error(f"Model training failed: {e}")
            raise
    
    def _train_single_model(self, model_type: ModelType, model: nn.Module,
                          train_loader: DataLoader, val_loader: DataLoader) -> TrainingResult:
        """Train a single model"""
        try:
            criterion = nn.CrossEntropyLoss()
            optimizer = self.optimizers[model_type]
            scheduler = self.schedulers.get(model_type)
            
            training_losses = []
            validation_losses = []
            best_val_loss = float('inf')
            patience_counter = 0
            best_epoch = 0
            
            start_time = time.time()
            
            for epoch in range(self.config.epochs):
                # Training phase
                model.train()
                train_loss = 0.0
                for batch_X, batch_y in train_loader:
                    batch_X = batch_X.to(self.device)
                    batch_y = batch_y.to(self.device).long().squeeze()
                    
                    optimizer.zero_grad()
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    
                    train_loss += loss.item()
                
                # Validation phase
                model.eval()
                val_loss = 0.0
                all_preds = []
                all_targets = []
                
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        batch_X = batch_X.to(self.device)
                        batch_y = batch_y.to(self.device).long().squeeze()
                        
                        outputs = model(batch_X)
                        loss = criterion(outputs, batch_y)
                        val_loss += loss.item()
                        
                        preds = torch.argmax(outputs, dim=1)
                        all_preds.extend(preds.cpu().numpy())
                        all_targets.extend(batch_y.cpu().numpy())
                
                # Calculate metrics
                train_loss /= len(train_loader)
                val_loss /= len(val_loader)
                
                training_losses.append(train_loss)
                validation_losses.append(val_loss)
                
                # Calculate accuracy metrics
                accuracy = np.mean(np.array(all_preds) == np.array(all_targets))
                precision, recall, f1 = self._calculate_classification_metrics(all_preds, all_targets)
                
                # Learning rate scheduling
                if scheduler:
                    scheduler.step(val_loss)
                
                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_epoch = epoch
                    patience_counter = 0
                    # Save best model
                    torch.save(model.state_dict(), f"best_{model_type.value}_model.pth")
                else:
                    patience_counter += 1
                
                if patience_counter >= self.config.early_stopping_patience:
                    logger.info(f"Early stopping triggered for {model_type.value}")
                    break
                
                if epoch % 10 == 0:
                    logger.info(f"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}")
            
            # Load best model
            model.load_state_dict(torch.load(f"best_{model_type.value}_model.pth"))
            
            training_time = time.time() - start_time
            
            # Calculate feature importance (simplified)
            feature_importance = self._calculate_feature_importance(model, val_loader)
            
            result = TrainingResult(
                model_type=model_type,
                training_loss=training_losses,
                validation_loss=validation_losses,
                accuracy=accuracy,
                precision=precision,
                recall=recall,
                f1_score=f1,
                training_time=training_time,
                best_epoch=best_epoch,
                feature_importance=feature_importance
            )
            
            logger.info(f"{model_type.value} training completed: Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
            return result
            
        except Exception as e:
            logger.error(f"Single model training failed for {model_type.value}: {e}")
            raise
    
    def _calculate_classification_metrics(self, preds: List[int], targets: List[int]) -> Tuple[float, float, float]:
        """Calculate precision, recall, and F1 score"""
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        try:
            precision = precision_score(targets, preds, average='weighted', zero_division=0)
            recall = recall_score(targets, preds, average='weighted', zero_division=0)
            f1 = f1_score(targets, preds, average='weighted', zero_division=0)
            
            return precision, recall, f1
            
        except Exception as e:
            logger.warning(f"Metrics calculation failed: {e}")
            return 0.0, 0.0, 0.0
    
    def _calculate_feature_importance(self, model: nn.Module, val_loader: DataLoader) -> Dict[str, float]:
        """Calculate feature importance using permutation importance"""
        try:
            model.eval()
            baseline_accuracy = 0.0
            all_preds = []
            all_targets = []
            
            # Calculate baseline accuracy
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X = batch_X.to(self.device)
                    batch_y = batch_y.to(self.device).long().squeeze()
                    
                    outputs = model(batch_X)
                    preds = torch.argmax(outputs, dim=1)
                    all_preds.extend(preds.cpu().numpy())
                    all_targets.extend(batch_y.cpu().numpy())
            
            baseline_accuracy = np.mean(np.array(all_preds) == np.array(all_targets))
            
            # Permutation importance for each feature
            feature_importance = {}
            for i, feature_name in enumerate(self.feature_names[:10]):  # Limit to first 10 features
                permuted_accuracy = 0.0
                permuted_preds = []
                
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        # Permute feature
                        batch_X_permuted = batch_X.clone()
                        batch_X_permuted[:, :, i] = batch_X_permuted[:, :, i][torch.randperm(batch_X_permuted.size(0))]
                        
                        batch_X_permuted = batch_X_permuted.to(self.device)
                        batch_y = batch_y.to(self.device).long().squeeze()
                        
                        outputs = model(batch_X_permuted)
                        preds = torch.argmax(outputs, dim=1)
                        permuted_preds.extend(preds.cpu().numpy())
                
                permuted_accuracy = np.mean(np.array(permuted_preds) == np.array(all_targets))
                importance = baseline_accuracy - permuted_accuracy
                feature_importance[feature_name] = max(0.0, importance)
            
            return feature_importance
            
        except Exception as e:
            logger.warning(f"Feature importance calculation failed: {e}")
            return {}
    
    def _calculate_ensemble_weights(self, training_results: Dict[ModelType, TrainingResult]) -> None:
        """Calculate ensemble weights based on model performance"""
        try:
            total_performance = 0.0
            for model_type, result in training_results.items():
                # Use F1 score as performance metric
                performance = result.f1_score
                self.ensemble_weights[model_type] = performance
                total_performance += performance
            
            # Normalize weights
            if total_performance > 0:
                for model_type in self.ensemble_weights:
                    self.ensemble_weights[model_type] /= total_performance
            else:
                # Equal weights if no performance data
                equal_weight = 1.0 / len(self.ensemble_weights)
                for model_type in self.ensemble_weights:
                    self.ensemble_weights[model_type] = equal_weight
            
            logger.info(f"Ensemble weights: {self.ensemble_weights}")
            
        except Exception as e:
            logger.error(f"Ensemble weight calculation failed: {e}")
            # Set equal weights as fallback
            equal_weight = 1.0 / len(self.models)
            for model_type in self.models:
                self.ensemble_weights[model_type] = equal_weight
    
    def predict(self, current_data: pd.DataFrame) -> TradingSignal:
        """Generate trading signal using ensemble prediction"""
        try:
            with self._lock:
                # Prepare features
                processed_data = self.prepare_features(current_data)
                
                # Get the most recent sequence
                feature_cols = self.feature_names + ['close', 'volume']
                features = processed_data[feature_cols].values
                
                if len(features) < self.config.sequence_length:
                    raise ValueError("Insufficient data for prediction")
                
                # Get the sequence
                sequence = features[-self.config.sequence_length:]
                
                # Scale features
                sequence_scaled = self.scalers['feature'].transform(sequence)
                sequence_tensor = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(self.device)
                
                # Ensemble prediction
                ensemble_probs = np.zeros(3)  # 3 classes: SELL, HOLD, BUY
                
                for model_type, model in self.models.items():
                    model.eval()
                    with torch.no_grad():
                        output = model(sequence_tensor)
                        probs = output.cpu().numpy()[0]
                        weight = self.ensemble_weights.get(model_type, 0.0)
                        ensemble_probs += probs * weight
                
                # Get predicted action and confidence
                predicted_class = np.argmax(ensemble_probs)
                confidence = ensemble_probs[predicted_class]
                
                # Map to action
                if predicted_class == 0:  # SELL
                    action = Action.SELL
                elif predicted_class == 2:  # BUY
                    action = Action.BUY
                else:  # HOLD
                    action = Action.HOLD
                
                # Only generate signal if confidence is high enough
                if confidence < self.config.min_confidence:
                    action = Action.HOLD
                    confidence = 0.0
                
                # Calculate stop loss and take profit
                current_price = current_data['close'].iloc[-1]
                atr = current_data['ATR'].iloc[-1] if 'ATR' in current_data.columns else current_price * 0.01
                
                if action == Action.BUY:
                    stop_loss = current_price - (atr * 2)
                    take_profit = current_price + (atr * 2 * self.config.risk_reward_ratio)
                elif action == Action.SELL:
                    stop_loss = current_price + (atr * 2)
                    take_profit = current_price - (atr * 2 * self.config.risk_reward_ratio)
                else:
                    stop_loss = take_profit = current_price
                
                signal = TradingSignal(
                    symbol="EUR/USD",  # Could be parameterized
                    action=action,
                    confidence=confidence,
                    price=current_price,
                    timestamp=datetime.now(),
                    stop_loss=stop_loss,
                    take_profit=take_profit,
                    model_type=self.config.model_type,
                    features_used=self.feature_names,
                    metadata={
                        'ensemble_probs': ensemble_probs.tolist(),
                        'ensemble_weights': self.ensemble_weights,
                        'atr_value': atr
                    }
                )
                
                logger.info(f"Generated signal: {action.value} (confidence: {confidence:.3f})")
                return signal
                
        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            # Return hold signal on error
            return TradingSignal(
                symbol="EUR/USD",
                action=Action.HOLD,
                confidence=0.0,
                price=0.0,
                timestamp=datetime.now(),
                stop_loss=0.0,
                take_profit=0.0,
                model_type=self.config.model_type,
                features_used=[],
                metadata={'error': str(e)}
            )
    
    async def predict_async(self, current_data: pd.DataFrame) -> TradingSignal:
        """Async version of predict method"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self._executor, self.predict, current_data)
    
    def get_model_performance(self) -> Dict[str, Any]:
        """Get model performance metrics"""
        try:
            performance = {}
            
            for model_type, result in self.training_history.items():
                performance[model_type.value] = {
                    'accuracy': result.accuracy,
                    'precision': result.precision,
                    'recall': result.recall,
                    'f1_score': result.f1_score,
                    'training_time': result.training_time,
                    'best_epoch': result.best_epoch
                }
            
            performance['ensemble_weights'] = self.ensemble_weights
            performance['feature_importance'] = {}
            
            # Combine feature importance from all models
            for model_type, result in self.training_history.items():
                for feature, importance in result.feature_importance.items():
                    if feature not in performance['feature_importance']:
                        performance['feature_importance'][feature] = 0.0
                    performance['feature_importance'][feature] += importance
            
            # Sort features by importance
            performance['feature_importance'] = dict(
                sorted(performance['feature_importance'].items(), 
                      key=lambda x: x[1], reverse=True)
            )
            
            return performance
            
        except Exception as e:
            logger.error(f"Performance retrieval failed: {e}")
            return {}
    
    def save_models(self, directory: str = "models") -> None:
        """Save trained models and scalers"""
        try:
            Path(directory).mkdir(exist_ok=True)
            
            # Save models
            for model_type, model in self.models.items():
                torch.save(model.state_dict(), f"{directory}/{model_type.value}_model.pth")
            
            # Save scalers
            joblib.dump(self.scalers, f"{directory}/scalers.pkl")
            
            # Save configuration
            config_data = {
                'model_config': self.config.__dict__,
                'feature_names': self.feature_names,
                'ensemble_weights': self.ensemble_weights,
                'training_history': {
                    k.value: v.__dict__ for k, v in self.training_history.items()
                }
            }
            
            with open(f"{directory}/config.json", 'w') as f:
                json.dump(config_data, f, indent=2, default=str)
            
            logger.info(f"Models saved to {directory}")
            
        except Exception as e:
            logger.error(f"Model saving failed: {e}")
            raise
    
    def load_models(self, directory: str = "models") -> None:
        """Load trained models and scalers"""
        try:
            # Load configuration
            with open(f"{directory}/config.json", 'r') as f:
                config_data = json.load(f)
            
            # Load scalers
            self.scalers = joblib.load(f"{directory}/scalers.pkl")
            self.feature_names = config_data['feature_names']
            self.ensemble_weights = {
                ModelType(k): v for k, v in config_data['ensemble_weights'].items()
            }
            
            # Initialize models
            input_size = len(self.feature_names) + 2  # +2 for close and volume
            self._initialize_models(input_size)
            
            # Load model weights
            for model_type, model in self.models.items():
                model.load_state_dict(torch.load(f"{directory}/{model_type.value}_model.pth"))
            
            logger.info("Models loaded successfully")
            
        except Exception as e:
            logger.error(f"Model loading failed: {e}")
            raise

# Example usage and testing
def main():
    """Example usage of the AdvancedDeepLearningStrategy"""
    
    # Generate sample market data
    print("=== Generating Sample Market Data ===")
    np.random.seed(42)
    
    dates = pd.date_range(start='2020-01-01', end='2023-01-01', freq='1H')
    n_periods = len(dates)
    
    # Create realistic price data with trends and volatility
    returns = np.random.normal(0.0001, 0.005, n_periods)
    prices = 1.1000 * np.exp(np.cumsum(returns))
    
    # Add some trends and patterns
    trend = np.sin(np.arange(n_periods) * 2 * np.pi / 1000) * 0.002
    prices += trend
    
    market_data = pd.DataFrame({
        'timestamp': dates,
        'open': prices * 0.999,
        'high': prices * 1.001,
        'low': prices * 0.998,
        'close': prices,
        'volume': np.random.lognormal(10, 1, n_periods)
    })
    
    print(f"Generated {len(market_data)} periods of market data")
    
    # Configure deep learning strategy
    config = ModelConfig(
        model_type=ModelType.ENSEMBLE,
        sequence_length=60,
        prediction_horizon=5,
        hidden_size=64,
        num_layers=2,
        learning_rate=0.001,
        batch_size=32,
        epochs=50,  # Reduced for demo
        min_confidence=0.6
    )
    
    # Initialize strategy
    strategy = AdvancedDeepLearningStrategy(config)
    
    print("\n=== Training Deep Learning Models ===")
    
    # Train models
    training_results = strategy.train_models(market_data)
    
    print("\n=== Training Results ===")
    for model_type, result in training_results.items():
        print(f"{model_type.value}:")
        print(f"  Accuracy: {result.accuracy:.4f}")
        print(f"  F1 Score: {result.f1_score:.4f}")
        print(f"  Training Time: {result.training_time:.2f}s")
        print(f"  Best Epoch: {result.best_epoch}")
    
    print("\n=== Model Performance Summary ===")
    performance = strategy.get_model_performance()
    print("Ensemble Weights:")
    for model, weight in performance['ensemble_weights'].items():
        print(f"  {model}: {weight:.3f}")
    
    print("\nTop 5 Feature Importance:")
    top_features = list(performance['feature_importance'].items())[:5]
    for feature, importance in top_features:
        print(f"  {feature}: {importance:.4f}")
    
    print("\n=== Generating Trading Signals ===")
    
    # Generate signals on recent data
    recent_data = market_data.tail(1000)
    signals = []
    
    for i in range(10):  # Generate 10 sample signals
        data_subset = recent_data.head(800 + i)  # Simulate moving window
        signal = strategy.predict(data_subset)
        signals.append(signal)
        
        print(f"Signal {i+1}: {signal.action.value} (Confidence: {signal.confidence:.3f})")
        if signal.action != Action.HOLD:
            print(f"  Price: {signal.price:.4f}, SL: {signal.stop_loss:.4f}, TP: {signal.take_profit:.4f}")
    
    print("\n=== Signal Statistics ===")
    action_counts = {}
    for signal in signals:
        action = signal.action.value
        action_counts[action] = action_counts.get(action, 0) + 1
    
    for action, count in action_counts.items():
        print(f"{action}: {count} signals")
    
    # Save models
    strategy.save_models("trading_models")
    print("\n=== Models Saved ===")
    
    print("\n=== Deep Learning Strategy Test Completed ===")

if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    main()